{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MYZYxdhc2O-"
      },
      "source": [
        "# This jupyter notebook is 1 of 5 notebooks in building an AI model about detecting skin cancer and deploying that model via designing a web application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JouKR3pTBF_L"
      },
      "source": [
        "# *Online Dermatologists:* üì± üåê Diagnosing Skin Cancer through a Web Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPnF7JnsD6II"
      },
      "source": [
        "1 in 3 cancer patients have a form of skin cancer, making it the most prevalent form of cancer in the world. In the US alone, over 9,000 patients are diagnosed on a daily basis. Skin cancer hits rural and impoverished communities especially hard, as without access to professional healthcare workers and equipment, many cases go undetected, and proper care can't be administerd in time. Let's try to help out by creating a web app, that anyone can access through a phone or laptop, to have those suspicious moles checked out with ML!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DyTf3tXjwlm"
      },
      "source": [
        "In this project, we will be be diagnosing skin lesion images for signs of skin cancer. To perform this task, we'll be working with an array of machine learning methods and models. We'll also be developing a web app to deploy our machine learning models! From there, we'll employ some unsupervised ML tecnhiques for data visualizations and perform skin cancer image segmentation in addition to just classification!\n",
        "\n",
        "The general outline for this project is as follows:\n",
        "*   Notebook 1: Exploring Skin Cancer data and developing basic ML models with Computer Vision\n",
        "*   Notebooks 2 and 3: Developing more advanced ML models and deploying ML to a web app\n",
        "*   Notebook 4: Checking for bias in ML models performing skin cancer diagnosis\n",
        "*   Notebook 5: Exploring more advanced ML methods for skin cancer diagosis and lesion segmentation\n",
        "\n",
        "In this notebook we'll be:\n",
        "*   Understanding our dataset\n",
        "*   Performing data preprocessing\n",
        "*   Learning how to manipulate images with OpenCV\n",
        "*   Artificially increasing our dataset's size\n",
        "*   Creating basic ML models with our dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ObbA3WmI798"
      },
      "source": [
        "# Understanding our Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZHIjKMBJEtP"
      },
      "source": [
        "Our dataset contains over 10,000 skin lesion images that fall into one of seven classes. These classes are melanocytic nevus, melanoma, benign keratosis, basal cell carcionoma, actinic keratosis, dermatofibroma, and vascular lesions.\n",
        "\n",
        "*   Melanocytic Nevus is the medical term used to denote a mole that originates from the melanocytes in the skin. These are harmless artifacts found on the skin.\n",
        "\n",
        "*   Melanoma is a very serious form of skin cancer that originates from melanocyctes, cells in skin that produce melanin.\n",
        "\n",
        "*   Benign Keratoses or Seborrheic Keratoses are skin artifacts that are not cancerous but form due to aging.\n",
        "\n",
        "*   Basal Cell Carcinoma is a common form of skin cancer that originates from the basal cells. These cells replace the skin cells that die off.\n",
        "\n",
        "*   Actinic Keratoses (Bowen's Disease) are a form of skin lesions that originate due to old age and sun exposure. These lesions are considered to be \"pre-cancerous\" and can develop to be cancerous.\n",
        "\n",
        "*   Dermatofibroma are harmless skin bumps that form due to an overgrowth of various skin cells.\n",
        "\n",
        "*   Vascular lesions are skin artifiacts often referred to as birthmarks. These lesions appear due to clustering of blood vessels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC-jSiP8bpo_"
      },
      "source": [
        "![alt text](https://workshop2018.isic-archive.com/images/task3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlTcpNp3bthz"
      },
      "source": [
        "Our images are sourced from the HAM10000 dataset which is publically available. Each image contains RGB data and is of the pixel dimensions 800 x 600. The images in the dataset are collected from a dermoscope, a tool that is used by dermatologists to image skin lesions. A dermoscope enhances images by providing maginification and adequate lighting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVvP2jVpcLQP"
      },
      "source": [
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/e/e6/Dermatoscope1.JPG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih8pc-WDIxbx"
      },
      "source": [
        "#@title Run this to download data and prepare our environment! { display-mode: \"form\" }\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D\n",
        "from keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import gdown\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D\n",
        "from keras.layers.merge import add, concatenate\n",
        "from keras.models import Model\n",
        "import struct\n",
        "from google.colab.patches import cv2_imshow\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "\n",
        "!pip install hypopt\n",
        "from hypopt import GridSearch\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "!pip install -U opencv-contrib-python\n",
        "import cv2\n",
        "\n",
        "!pip install tensorflowjs\n",
        "import tensorflowjs as tfjs\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import requests, io, zipfile\n",
        "\n",
        "# Prepare data\n",
        "\n",
        "images_1 = os.makedirs('images_1', exist_ok=True)\n",
        "images_2= os.makedirs('images_2', exist_ok=True)\n",
        "images_all= os.makedirs('images_all', exist_ok=True)\n",
        "\n",
        "metadata_path = 'metadata.csv'\n",
        "image_path_1 = 'images_1.zip'\n",
        "image_path_2 = 'images_2.zip'\n",
        "images_rgb_path = 'hmnist_8_8_RGB.csv'\n",
        "\n",
        "!wget -O metadata.csv 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20B)%20Skin%20Cancer%20Diagnosis/metadata.csv'\n",
        "!wget -O images_1.zip 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20B)%20Skin%20Cancer%20Diagnosis/images_1.zip'\n",
        "!wget -O images_2.zip 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20B)%20Skin%20Cancer%20Diagnosis/images_2.zip'\n",
        "!wget -O hmnist_8_8_RGB.csv 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20B)%20Skin%20Cancer%20Diagnosis/hmnist_8_8_RGB.csv'\n",
        "!unzip -q -o images_1.zip -d images_1\n",
        "!unzip -q -o images_2.zip -d images_2\n",
        "\n",
        "!pip install patool\n",
        "import patoolib\n",
        "\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "fromDirectory = 'images_1'\n",
        "toDirectory = 'images_all'\n",
        "\n",
        "copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "fromDirectory = 'images_2'\n",
        "toDirectory = 'images_all'\n",
        "\n",
        "copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "print(\"Downloaded Data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyvhWIyIdaqF"
      },
      "source": [
        "# Preparing Our Dataset for Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITfh4GjlttRH"
      },
      "source": [
        "IMG_WIDTH = 100\n",
        "IMG_HEIGHT = 75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0E6zq46LABa"
      },
      "source": [
        "We'll start off by separating our dataset into the `X` and `y` variables. `X` represents our input data (images), and `y` represents our data's labels (skin lesion classification). Each image is scaled down to be 100 px by 75 px to reduce the memory footprint. We'll also create a variable `X_gray`, that is the grayscale equivalent of our `X` variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkabv4pVmG0M"
      },
      "source": [
        "One reason for performing these grayscale transformations could be to reduce bias in a classifier. This could prevent the ML model from becoming dependent on the color of the skin, as opposed to the features present in the actual skin cancer lesion. Another reason could lie with the need to reduce the dimensionality of our dataset for our simple ML classifiers we'll train later on. The less complex the data is for training, the less likely our models is to overfit on the data. By performing this grayscale operation, we're reducing our RGB values for each pixel into one grayscale value from 0 to 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLW3gPKVk-I-"
      },
      "source": [
        "As there are over 10,000 images, this code segment may take a few minutes to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSQRAESRah1H"
      },
      "source": [
        "X = []\n",
        "X_gray = []\n",
        "y = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX1w_FIuv26r"
      },
      "source": [
        "#@title Run this to initialize our X, X_gray, and y variables { display-mode: \"form\" }\n",
        "metadata = pd.read_csv(metadata_path)\n",
        "metadata['category'] = metadata['dx'].replace({'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'nv': 5, 'vasc': 6,})\n",
        "\n",
        "\n",
        "for i in tqdm(range(len(metadata))):\n",
        "  image_meta = metadata.iloc[i]\n",
        "  path = os.path.join(toDirectory, image_meta['image_id'] + '.jpg')\n",
        "  img = cv2.imread(path,cv2.IMREAD_COLOR)\n",
        "  img = cv2.resize(img,(IMG_WIDTH,IMG_HEIGHT))\n",
        "\n",
        "  img_g = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "  X_gray.append(img_g)\n",
        "\n",
        "  X.append(img)\n",
        "  y.append(image_meta['category'])\n",
        "\n",
        "X_gray = np.array(X_gray)\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDGuV3sTmq2d"
      },
      "source": [
        "Let's take a look at an example of what our data looks like! Let's explore the dataset for different indicies in our `X` variable!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj6esISFmwvc"
      },
      "source": [
        "cv2_imshow(X[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VSd4yTLeGLZ"
      },
      "source": [
        "Let's take a look at the shape of our updated `X`, `X_gray`, and `y` variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbl7SxZ2ynSF"
      },
      "source": [
        "print(X_gray.shape)\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzUwppWqjJjJ"
      },
      "source": [
        "It looks like we've got a total of 10,015 images in our dataset. Plotting a graph of the distribution of labels found in the dataset can help us determine if we need to balance the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPOCn5CJjWIJ"
      },
      "source": [
        "#@title Run this to plot the distribution of our dataset { display-mode: \"form\" }\n",
        "objects = ('akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc')\n",
        "y_pos = np.arange(len(objects))\n",
        "occurances = []\n",
        "\n",
        "for obj in objects:\n",
        "  occurances.append(np.count_nonzero(obj == metadata['dx']))\n",
        "\n",
        "print(occurances)\n",
        "\n",
        "plt.bar(y_pos, occurances, align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, objects)\n",
        "plt.ylabel('Samples')\n",
        "plt.title('Distribution of Classes Within Dataset')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lseeMJSlma2B"
      },
      "source": [
        "This bar chart clearly informs us that our dataset is very unbalanced. There are far more nevi samples than there are samples of any other class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhtPLPEUMKCe"
      },
      "source": [
        "For the sake of reducing execution and training time, we'll be cutting down the size of our dataset. To observe the full performance of our model and see the complete extent of our visualizations, we can comment out the following lines and re-run the notebook. However, note that some code blocks will take much longer to run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7z0n6PuoItk"
      },
      "source": [
        "We can decide between the following methods we can use to reduce our dataset size. We only run one of the two code blocks. The first option reduces the dataset size far more than the second option. Specify which option you would like to proceed with by setting the value for the variable `option`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yvxpG1LB2Ig"
      },
      "source": [
        "sample_cap = 142\n",
        "option = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAZqqMTgnmOq"
      },
      "source": [
        "#@title Option 1: Run this to reduce dataset size. This method caps each class at *sample_cap* samples. { display-mode: \"form\" }\n",
        "if (option == 1):\n",
        "  objects = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
        "  class_totals = [0,0,0,0,0,0,0]\n",
        "  iter_samples = [0,0,0,0,0,0,0]\n",
        "  indicies = []\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    class_totals[y[i]] += 1\n",
        "\n",
        "  print(\"Initial Class Samples\")\n",
        "  print(class_totals)\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    if iter_samples[y[i]] != sample_cap:\n",
        "      indicies.append(i)\n",
        "      iter_samples[y[i]] += 1\n",
        "\n",
        "  X = X[indicies]\n",
        "  X_gray = X_gray[indicies]\n",
        "\n",
        "  y = y[indicies]\n",
        "\n",
        "  class_totals = [0,0,0,0,0,0,0]\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    class_totals[y[i]] += 1\n",
        "\n",
        "  print(\"Modified Class Samples\")\n",
        "  print(class_totals)\n",
        "else:\n",
        "  print(\"This option was not selected\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H2PgS99nGdZ"
      },
      "source": [
        "#@title Option 2: Run this to reduce dataset size. This method only reduces the number of *nv* samples to be the same amount as the number of samples found in the second most prevalent class. { display-mode: \"form\" }\n",
        "if (option == 2):\n",
        "  objects = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
        "  class_totals = [0,0,0,0,0,0,0]\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    class_totals[y[i]] += 1\n",
        "\n",
        "  print(\"Initial Class Samples\")\n",
        "  print(class_totals)\n",
        "\n",
        "  largest_index = class_totals.index(max(class_totals))\n",
        "  class_totals[largest_index] = 0\n",
        "\n",
        "  second_largest_val = max(class_totals)\n",
        "\n",
        "  indicies = []\n",
        "  iter = 0\n",
        "  for i in range(len(X)):\n",
        "    if y[i] == largest_index:\n",
        "      if iter != second_largest_val:\n",
        "        indicies.append(i)\n",
        "        iter += 1\n",
        "      else:\n",
        "        continue\n",
        "    else:\n",
        "      indicies.append(i)\n",
        "\n",
        "  class_totals = [0,0,0,0,0,0,0]\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    class_totals[y[i]] += 1\n",
        "\n",
        "  print(\"Modified Class Samples\")\n",
        "  print(class_totals)\n",
        "\n",
        "  X = X[indicies]\n",
        "  X_gray = X_gray[indicies]\n",
        "\n",
        "  y = y[indicies]\n",
        "else:\n",
        "  print(\"This option was not selected\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijzkxsCGa86d"
      },
      "source": [
        "By running the second code block above, our dataset is no longer imbalanced. This would mean that we could use accuracy as a metric for performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ry-_xQYmAjr"
      },
      "source": [
        "# OpenCV Image Manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-ZCjdBIt_zO"
      },
      "source": [
        "Having a large and rich dataset allows our model to be exposed to different types of images and in turn, perform better when given images to classify."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnu_gUSBvFya"
      },
      "source": [
        "Consider professional images taken with proper medical equipment by a dermatologist. These images are more likely to be clearer and in focus, when compared with those taken by an amateur with a cell phone camera. However, as both types of images are likely to be sent to our ML model for classification, its important that we prepare our model for both situations.\n",
        "\n",
        "One method of increasing our dataset's size is called *data augmentation*. Through data augmentation, we take existing images from our dataset, and duplicate a version of that image with an image transformation applied to it. This process can be repeated multiple times, and the dataset size can increase ten-fold or greater. Well, what does this mean in practice?\n",
        "\n",
        "Let's explore this further with the example of this *Jaguar* sports car."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyjerGg2wYxT"
      },
      "source": [
        "#@title Run this to download our Jaguar car image! { display-mode: \"form\" }\n",
        "!wget -O jaguar.jpeg 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20B)%20Skin%20Cancer%20Diagnosis/jaguar.jpeg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9M_04-zxAyy"
      },
      "source": [
        "jaguar = cv2.imread(\"jaguar.jpeg\")\n",
        "cv2_imshow(jaguar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5Oo4w1sxywa"
      },
      "source": [
        "This is a very crisp and detailed image. But what if in the future, after the model has been trained on numerous images like this, it's presented with an image of lower quality? What if the camera wasn't completely in focus?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhhOv9tFbiku"
      },
      "source": [
        "One solution is to artificially generate these lower quality pictures ourselves. This way we could expose the model to lower quality images and prepare it in case it recieve similar images in the future.\n",
        "\n",
        "To manipulate and edit images, we'll be exploring the use of the OpenCV library. OpenCV is a very powerful image processing library that has the capability to transform images in numerous ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PcPlhWNzcOH"
      },
      "source": [
        "Here are two functions in OpenCV:\n",
        "\n",
        "\n",
        "\n",
        "*   `cv2.resize(image,(new_width,new_height))` resizes an image.\n",
        "*   `cv2.blur(image,(kernel_size,kernel_size))` blurs an image. The `kernel_size` argument indicates how wide and how high the window that smoothes the image is. The larger the kernel size, the more intense the blur.\n",
        "\n",
        "Also, we can use `cv2_imshow()` to view our images in Colab. If not in a Colab ntoebook environment, we can use `cv2.imshow()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCLuB0DHzlNy"
      },
      "source": [
        "# Blur\n",
        "blur_jaguar = cv2.blur(jaguar,(4,4))\n",
        "cv2_imshow(blur_jaguar)\n",
        "\n",
        "# Resize\n",
        "small_jaguar = cv2.resize(jaguar,(455,256))\n",
        "normal_jaguar = cv2.resize(small_jaguar,(910,511))\n",
        "cv2_imshow(normal_jaguar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOtZKTm2rTSd"
      },
      "source": [
        "Let's say that our classifier was comparing between Ferraris and Jaguars. and in most of the training data, the Ferraris were red and the Jaguars were black."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-bOA03EsCSI"
      },
      "source": [
        "OpenCV uses the `BGR` coloring scheme as opposed to the traditional `RGB` coloring scheme for its images. This means that the first element of each pixel is the blue channel, the second element is the green channel, and the third element is the red channel. The function `cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)` converts an image to black and white, while the function `cv2.flip(image,i)` flips an image. When `i` is `0`,`1`,or `-1`, the image is flipped against the x-axis, y-axis, or against both axes respectively. We can try creating some images that could reduce the likelihood of the aforementioned errors occuring:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2bsX-KxtwTC"
      },
      "source": [
        "# Grayscale\n",
        "jaguar_bw = cv2.cvtColor(jaguar,cv2.COLOR_BGR2GRAY)\n",
        "cv2_imshow(jaguar_bw)\n",
        "\n",
        "# Flip\n",
        "jaguar_flip = cv2.flip(jaguar,0)\n",
        "cv2_imshow(jaguar_flip)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXwpEoFBveXl"
      },
      "source": [
        "Another image transformation we can implement is a *zoom*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mr74uBxGy-PS"
      },
      "source": [
        "#Zoom into our image\n",
        "zoom = 0.33\n",
        "\n",
        "centerX,centerY=int(jaguar.shape[0]/2),int(jaguar.shape[1]/2)\n",
        "radiusX,radiusY= int((1-zoom)*jaguar.shape[0]*2),int((1-zoom)*jaguar.shape[1]*2)\n",
        "\n",
        "minX,maxX=centerX-radiusX,centerX+radiusX\n",
        "minY,maxY=centerY-radiusY,centerY+radiusY\n",
        "\n",
        "cropped = jaguar[minX:maxX, minY:maxY]\n",
        "zoom_img = cv2.resize(cropped, (jaguar.shape[1], jaguar.shape[0]))\n",
        "cv2_imshow(zoom_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Pl_NAuwsFAT"
      },
      "source": [
        "Now, we've explored many different image operations we can perfom using OpenCV. Now, let's head back to our skin cancer image dataset, and apply what we've learned there!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA4dVOMFsQCB"
      },
      "source": [
        "# Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uEU-wxbsTJT"
      },
      "source": [
        "Although our dataset is very expansive with over 10,000 images, we can generate more samples so that our model is prepared to cope with a more varied dataset. Through data augmentation, we can perform random operations such as a flip, blur, or zoom on existing images, to create new image samples. It's important to note that these data augmentation procedures should only be applied to the training dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E85zF4vc1qV8"
      },
      "source": [
        "**Note:** Everything except for converting to grayscale and thresholding/segmentation should work (for color images) well with our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1_Vt1ghOfpJ"
      },
      "source": [
        "Let's first complete our test/train split for both our grayscale image data and our color image data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgcte9wXU2NJ"
      },
      "source": [
        "X_gray_train, X_gray_test, y_train, y_test = train_test_split(X_gray, y, test_size=0.4, random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu6T_f60B_Qm"
      },
      "source": [
        "Let's also perform a test/train split for `X` and `y`: the color image data and the respective labels. We need to create `X_train, X_test, y_train, y_test`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ISotMsZCBXz"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnHgKvsevtyA"
      },
      "source": [
        "We'll now iterate through all the images in the training slice of our dataset and create a duplicate with a random transformation, doubling our training dataset's size. In this code block, we'll randomly decide to flip the image across the y-axis or apply a 33% zoom."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC4VupwgyqpN"
      },
      "source": [
        "X_augmented = []\n",
        "X_gray_augmented = []\n",
        "\n",
        "y_augmented = []\n",
        "\n",
        "for i in tqdm(range(len(X_train))):\n",
        "  transform = random.randint(0,1)\n",
        "  if (transform == 0):\n",
        "    # Flip the image across the y-axis\n",
        "    X_augmented.append(cv2.flip(X_train[i],1))\n",
        "    X_gray_augmented.append(cv2.flip(X_gray_train[i],1))\n",
        "    y_augmented.append(y_train[i])\n",
        "  else:\n",
        "    # Zoom 33% into the image\n",
        "    zoom = 0.33\n",
        "\n",
        "    centerX,centerY=int(IMG_HEIGHT/2),int(IMG_WIDTH/2)\n",
        "    radiusX,radiusY= int((1-zoom)*IMG_HEIGHT*2),int((1-zoom)*IMG_WIDTH*2)\n",
        "\n",
        "    minX,maxX=centerX-radiusX,centerX+radiusX\n",
        "    minY,maxY=centerY-radiusY,centerY+radiusY\n",
        "\n",
        "    cropped = (X_train[i])[minX:maxX, minY:maxY]\n",
        "    new_img = cv2.resize(cropped, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    X_augmented.append(new_img)\n",
        "\n",
        "    cropped = (X_gray_train[i])[minX:maxX, minY:maxY]\n",
        "    new_img = cv2.resize(cropped, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    X_gray_augmented.append(new_img)\n",
        "\n",
        "    y_augmented.append(y_train[i])\n",
        "\n",
        "X_augmented = np.array(X_augmented)\n",
        "X_gray_augmented = np.array(X_gray_augmented)\n",
        "\n",
        "y_augmented = np.array(y_augmented)\n",
        "\n",
        "X_train = np.vstack((X_train,X_augmented))\n",
        "X_gray_train = np.vstack((X_gray_train,X_gray_augmented))\n",
        "\n",
        "y_train = np.append(y_train,y_augmented)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOcxQBOiRcVq"
      },
      "source": [
        "#@title Run this to Combine Augmented Data with Existing Samples { display-mode: \"form\" }\n",
        "X_augmented = np.array(X_augmented)\n",
        "X_gray_augmented = np.array(X_gray_augmented)\n",
        "\n",
        "y_augmented = np.array(y_augmented)\n",
        "\n",
        "X_train = np.vstack((X_train,X_augmented))\n",
        "X_gray_train = np.vstack((X_gray_train,X_gray_augmented))\n",
        "\n",
        "y_train = np.append(y_train,y_augmented)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwVcrhcDDE7h"
      },
      "source": [
        "Let's view the shape of our training variables after data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFBIEeze7gCD"
      },
      "source": [
        "print(X_gray_train.shape)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U18ftDzmcI_3"
      },
      "source": [
        "We can try performing two additional image transformations with OpenCV for data augmentation!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhgcZ49cvsQn"
      },
      "source": [
        "X_augmented = []\n",
        "X_gray_augmented = []\n",
        "\n",
        "y_augmented = []\n",
        "\n",
        "for i in tqdm(range(len(X_train))):\n",
        "  transform = random.randint(0,1)\n",
        "  if (transform == 0):\n",
        "\n",
        "    # Resize the image by half on each dimension, and resize back to original\n",
        "    # dimensions\n",
        "\n",
        "    small_image = cv2.resize(X_train[i],(IMG_WIDTH//2,IMG_HEIGHT//2))\n",
        "    normal_image = cv2.resize(small_image,(IMG_WIDTH,IMG_HEIGHT))\n",
        "\n",
        "    small_grayscale_image = cv2.resize(X_gray_train[i],(IMG_WIDTH//2,IMG_HEIGHT//2))\n",
        "    normal_grayscale_image = cv2.resize(small_grayscale_image,(IMG_WIDTH,IMG_HEIGHT))\n",
        "\n",
        "    X_augmented.append(normal_image)\n",
        "    X_gray_augmented.append(normal_grayscale_image)\n",
        "    y_augmented.append(y_train[i])\n",
        "  else:\n",
        "\n",
        "    # Blur the image with a 4 x 4 kernel\n",
        "\n",
        "    X_augmented.append(cv2.blur(X_train[i],(4,4)))\n",
        "    X_gray_augmented.append(cv2.blur(X_gray_train[i],(4,4)))\n",
        "    y_augmented.append(y_train[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSMIu4eZ2UUK"
      },
      "source": [
        "#@title Run this to Combine Augmented Data with Existing Samples { display-mode: \"form\" }\n",
        "X_augmented = np.array(X_augmented)\n",
        "X_gray_augmented = np.array(X_gray_augmented)\n",
        "\n",
        "y_augmented = np.array(y_augmented)\n",
        "\n",
        "X_train = np.vstack((X_train,X_augmented))\n",
        "X_gray_train = np.vstack((X_gray_train,X_gray_augmented))\n",
        "\n",
        "y_train = np.append(y_train,y_augmented)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZThMze80XRW"
      },
      "source": [
        "# Creating Basic Machine Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAFRpb9z0dpz"
      },
      "source": [
        "Now that we've implemented data augmentation into our pipeline and artificially generated more samples for our dataset, lets test out various ML models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgNc8o6P2eaw"
      },
      "source": [
        "Let's start off by creating a K Nearest Neighbors model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktDk1MSSViBN"
      },
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWmaiGm12pwu"
      },
      "source": [
        "Scikit-learn takes feature vectors as data samples (1D arrays). However, images have at least 2 dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMWLkexL_QZO"
      },
      "source": [
        "Let's perform an operation known as *image flattening* with our grayscale image data. In this operation, we reshape our images to be a one dimensional array of length 7500 instead of a matrix of dimensions (100 x 75)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F85wUJf1_MC2"
      },
      "source": [
        "X_g_train_flat = X_gray_train.reshape(X_gray_train.shape[0],-1)\n",
        "X_g_test_flat = X_gray_test.reshape(X_gray_test.shape[0],-1)\n",
        "print (X_g_train_flat.shape)\n",
        "print (X_g_test_flat.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GavYaRIaQYE"
      },
      "source": [
        "Let's train our models on our flattened grayscale images! Once again, due to the size of our dataset, training may take a few minutes for each model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S2bPvic4O3w"
      },
      "source": [
        "knn.fit(X_g_train_flat, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGSImVpxOnsz"
      },
      "source": [
        "A common way to measure our model's performance uses the Receiver Operator Curve, which shows the relationship between our model's true positive and true negative rate. This metric is especially useful with our scenario, since - unlike accuracy - it doesn't depend on balanced classes in our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV-GnwrfReGq"
      },
      "source": [
        "Here is an example of an ROC curve. It shows the true positive rate and true negative rate as we vary the postitive/negative threshold for a classifier. The AUC, or Area Under the Curve, is the metric we use. The greater the area - the closer to the top left the curve lies - the better the model. A model that guesses randomly would fit the 45 degree line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4lD9xXwd-aD"
      },
      "source": [
        "We'll define a function called `model_stats()` that prints the models performance. Specifically, it will print the model's name, its accuracy, and its ROC AUC value. Before we create our function, there's one more thing to cover.\n",
        "\n",
        "When we calculate the ROC AUC score, we have to compare the true test labels, `y_test`, against the predicted probabilities for each class for every sample. What does this mean? Let's take a look at an example!\n",
        "\n",
        "If a model was predicting class `0` in a classifier, the one hot repesentation of this would be `[1, 0, 0, 0]`. The probabilistic representation of this could be `[0.4, 0.2, 0.2, 0.2]`. All the probability values in the array add up to `1`. Each element of this array represents the probability of that specific class being predicted by the classifier. For example, the probability of class `0` being predicted is represented by the value in the zeroeth element of the array. In this case that would be `0.4`.\n",
        "\n",
        "We can calculate these probability arrays using the `predict_proba()` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeQoEnO40eDp"
      },
      "source": [
        "We can code for model_stats. To calculate ROC AUC scores, we can use the `roc_auc_score()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF-1_CXpaYnr"
      },
      "source": [
        "def model_stats(name, y_test, y_pred, y_pred_proba):\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  print(name)\n",
        "\n",
        "  accuracy = accuracy_score(y_test,y_pred)\n",
        "  print (\"The accuracy of the model is \" + str(round(accuracy,5)))\n",
        "\n",
        "  roc_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovo')\n",
        "\n",
        "  print (\"The ROC AUC Score of the model is \" + str(round(roc_score,5)))\n",
        "\n",
        "  return cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnpkxmD-gDF4"
      },
      "source": [
        "Let's run the function and observe the performance of our K Nearest Neighbors model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEBp2JwngLr6"
      },
      "source": [
        "y_pred = knn.predict(X_g_test_flat)\n",
        "y_pred_proba = knn.predict_proba(X_g_test_flat)\n",
        "\n",
        "knn_cm = model_stats(\"K Nearest Neighbors\",y_test,y_pred,y_pred_proba)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRDgkdKRqCy_"
      },
      "source": [
        "There seems to a big discrepancy between our accuracy and ROC AUC scores. Why is that? Let's take a look at some plots of the confusion matrices. Let's create a function called `plot_cm()`, that we will use to plot the confusion matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OsBLcK0qDEe"
      },
      "source": [
        "def plot_cm(name, cm):\n",
        "  classes = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
        "\n",
        "  df_cm = pd.DataFrame(cm, index = [i for i in classes], columns = [i for i in classes])\n",
        "  df_cm = df_cm.round(5)\n",
        "\n",
        "  plt.figure(figsize = (12,8))\n",
        "  sns.heatmap(df_cm, annot=True, fmt='g')\n",
        "  plt.title(name + \" Model Confusion Matrix\")\n",
        "  plt.xlabel(\"Predicted Label\")\n",
        "  plt.ylabel(\"True Label\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKcvTCSshxZl"
      },
      "source": [
        "Let's run our new function for KNN classifier. Remember that we have seven classes, so an accuracy that seems horribly low (like 50%) isn't as bad as it might appear!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_ghLFY0iI2D"
      },
      "source": [
        "plot_cm(\"K Nearest Neighbors\",knn_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6-5xtwWqY-f"
      },
      "source": [
        "It seems that while many nevi images were accurately classified, many other images of other classes were incorrectly classified as nevi. Due to our dataset being very imbalanced, the accuracy is misleading, as it is sensitive to imbalanced data. In addition, an AUC ROC score close to 0.5 indicates that the model is not capable of discriminating between the classes very well at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHYTkptEDUZd"
      },
      "source": [
        "Let's try modifying our KNN model's architecture and hyperparameters to increase our model's performance. We can use a library called *hypopt* to automate this process through a *grid search*. We'll automatically try out many possible hyperparameters for our machine learning algorithm to see which give the best performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vstr8QI-hg7X"
      },
      "source": [
        "Before we perform a grid search, we need to create an additional slice of our dataset. As of now, we have 60% of the data allocated for training, and 40% for testing. We'll now create a new slice of our dataset called the validation dataset. The validation dataset will be used for model testing during the grid search and will comprise 50% of our testing set, or 20% of the entire dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezEK7q4uEvTO"
      },
      "source": [
        "Let's make create the new variables `X_gray_test, X_gray_val, y_g_test, y_g_val`. Also let's create `X_gray_val_flat` and `X_gray_test_flat`, our flattened arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYZxsv0p82Ug"
      },
      "source": [
        "X_gray_test, X_gray_val, y_g_test, y_g_val = train_test_split(X_gray_test, y_test, test_size=0.5, random_state=101)\n",
        "\n",
        "X_gray_test_flat = np.reshape(X_gray_test,(X_gray_test.shape[0],X_gray_test.shape[1]*X_gray_test.shape[2]))\n",
        "X_gray_val_flat = np.reshape(X_gray_val,(X_gray_val.shape[0],X_gray_val.shape[1]*X_gray_val.shape[2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiQ_YWyduNfa"
      },
      "source": [
        "X_gray_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia1YhKF9EBN6"
      },
      "source": [
        "In the variable `param_grid` we can specify which parameters in our KNN Classifier we want to modify."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II5N-8lzEF5w"
      },
      "source": [
        "param_grid = {\n",
        "              'n_neighbors' :     [2, 3, 4, 5],\n",
        "              'weights' :          ['uniform', 'distance'],\n",
        "              'algorithm' :        ['ball_tree', 'kd_tree', 'brute']\n",
        "             }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlpSTulBFB0S"
      },
      "source": [
        "Let's initialize and fit our grid search optimizer. This can take a while!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J38TDZnwDwXa"
      },
      "source": [
        "gs_knn = GridSearch(model=KNeighborsClassifier(),param_grid=param_grid)\n",
        "\n",
        "gs_knn.fit(X_g_train_flat.astype(np.float32), y_train.astype(np.float32),\n",
        "       X_gray_val_flat.astype(np.float32), y_g_val.astype(np.float32),verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE_-4kZzE7as"
      },
      "source": [
        "Now, the model will be trained with the best hyperparameters. Let's try evaluating its performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUEEYEoGi5bu"
      },
      "source": [
        "y_pred = gs_knn.predict(X_gray_test_flat)\n",
        "y_pred_proba = gs_knn.predict_proba(X_gray_test_flat)\n",
        "gs_knn_cm = model_stats(\"Grid Search KNN\",y_g_test,y_pred,y_pred_proba)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYd8izH3FL5E"
      },
      "source": [
        "Let's also plot the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqcINkKmjLSF"
      },
      "source": [
        "plot_cm(\"Grid Search KNN\",gs_knn_cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sibU2QhZS7Y"
      },
      "source": [
        "Seems like the grid search didn't improve the model's performance. It could be that this ML model is unable to handle the dimensionality of our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvKLVAK5q2-d"
      },
      "source": [
        "Let's try out some of these other ML models as well. Perhaps some of these would perform better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPTxHzZjq7yO"
      },
      "source": [
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    SVC(kernel=\"linear\", C=0.025),\n",
        "    SVC(gamma=2, C=1),\n",
        "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
        "    DecisionTreeClassifier(max_depth=5),\n",
        "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "    MLPClassifier(alpha=1, max_iter=1000),\n",
        "    AdaBoostClassifier(),\n",
        "    GaussianNB(),\n",
        "    QuadraticDiscriminantAnalysis()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kknqLOr5juPB"
      },
      "source": [
        "That's a wrap for this notebook! In the next notebook, we'll create more complex ML models (which will hopefully work better!) and finally deploy our models to a web app."
      ]
    }
  ]
}