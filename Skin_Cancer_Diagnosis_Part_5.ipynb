{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKi9mw2ndMHE"
      },
      "source": [
        "# This jupyter notebook is 5 of 5 notebooks in building an AI model about detecting skin cancer and deploying that model via designing a web application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JouKR3pTBF_L"
      },
      "source": [
        "# *Online Dermatologists:* üì± üåê Diagnosing Skin Cancer through a Web Application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-KosQBh16cc"
      },
      "source": [
        "In this project, we will be be diagnosing skin lesion images for signs of skin cancer. To perform this task, we'll be working with an array of machine learning methods and models.\n",
        "\n",
        "In previous notebooks, we've created ML models to perform skin cancer image classification and deployed our models to web apps. Now, what are some ways we could improve our classification performance? Perhaps we could visualize our dataset to better understand some trends within it? Could we also explore alternative methods of performing classification, in addition to just CNNs and traditional Ml models? What if we could segment out the skin cancer lesion, to prevent our ML classification models from being tripped up by elements of the background?\n",
        "\n",
        "In this notebook we'll be:\n",
        "*   Visualizing our data with dimensionality reduction techniques\n",
        "*   Detecting Computer Vision features and using them for classification\n",
        "*   Performing Lesion Segmentation with ML processes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ObbA3WmI798"
      },
      "source": [
        "# Set up our Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih8pc-WDIxbx"
      },
      "source": [
        "#@title Run this to download data and prepare our environment! { display-mode: \"form\" }\n",
        "!pip install -U opencv-contrib-python\n",
        "import cv2\n",
        "\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D\n",
        "from keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import gdown\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D\n",
        "from keras.layers.merge import add, concatenate\n",
        "from keras.models import Model\n",
        "import struct\n",
        "from google.colab.patches import cv2_imshow\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "\n",
        "!pip install hypopt\n",
        "from hypopt import GridSearch\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "!pip install tensorflowjs\n",
        "import tensorflowjs as tfjs\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import requests, io, zipfile\n",
        "\n",
        "# Prepare data\n",
        "\n",
        "images_1 = os.makedirs('images_1', exist_ok=True)\n",
        "images_2= os.makedirs('images_2', exist_ok=True)\n",
        "images_all= os.makedirs('images_all', exist_ok=True)\n",
        "\n",
        "metadata_path = 'metadata.csv'\n",
        "image_path_1 = 'images_1.zip'\n",
        "image_path_2 = 'images_2.zip'\n",
        "images_rgb_path = 'hmnist_8_8_RGB.csv'\n",
        "\n",
        "!wget -O metadata.csv 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20B)%20Skin%20Cancer%20Diagnosis/metadata.csv'\n",
        "!wget -O images_1.zip 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20B)%20Skin%20Cancer%20Diagnosis/images_1.zip'\n",
        "!wget -O images_2.zip 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20B)%20Skin%20Cancer%20Diagnosis/images_2.zip'\n",
        "!wget -O hmnist_8_8_RGB.csv 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20B)%20Skin%20Cancer%20Diagnosis/hmnist_8_8_RGB.csv'\n",
        "!unzip -q -o images_1.zip -d images_1\n",
        "!unzip -q -o images_2.zip -d images_2\n",
        "\n",
        "!pip install patool\n",
        "import patoolib\n",
        "\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "fromDirectory = 'images_1'\n",
        "toDirectory = 'images_all'\n",
        "\n",
        "copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "fromDirectory = 'images_2'\n",
        "toDirectory = 'images_all'\n",
        "\n",
        "copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "os.makedirs(\"static/js\")\n",
        "!wget -O static/js/skin_cancer_diagnosis_script.js 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20B)%20Skin%20Cancer%20Diagnosis/skin_cancer_diagnosis_script.js'\n",
        "output = 'static/js/skin_cancer_diagnosis_script.js'\n",
        "\n",
        "print(\"Downloaded Data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW6Q5HorD4vn"
      },
      "source": [
        "#@title If the previous cell doesn't run, run this cell instead! { display-mode: \"form\" }\n",
        "!pip install -U opencv-contrib-python\n",
        "import cv2\n",
        "\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D\n",
        "from keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import gdown\n",
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D\n",
        "from keras.layers.merge import add, concatenate\n",
        "from keras.models import Model\n",
        "import struct\n",
        "from google.colab.patches import cv2_imshow\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.applications.mobilenet import MobileNet\n",
        "\n",
        "!pip install hypopt\n",
        "from hypopt import GridSearch\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "!pip install tensorflowjs\n",
        "import tensorflowjs as tfjs\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import requests, io, zipfile\n",
        "\n",
        "# Prepare data\n",
        "\n",
        "DATA_ROOT = '/content/data'\n",
        "os.makedirs(DATA_ROOT, exist_ok=True)\n",
        "\n",
        "metadata_url = 'https://drive.google.com/uc?id=1kmpa-Lnra-8KhEjj8r3nj9y4e53qcPGX'\n",
        "metadata_path = os.path.join(DATA_ROOT, 'metadata.csv')\n",
        "requests.get(metadata_url)\n",
        "gdown.download(metadata_url, metadata_path, True)\n",
        "\n",
        "images_1 = 'https://drive.google.com/uc?id=1HW5HbQ_OR7xUPWfw4yg1r_EbhDaCuOrj'\n",
        "image_path_1 = os.path.join(DATA_ROOT, 'images_1.zip')\n",
        "requests.get(images_1)\n",
        "gdown.download(images_1, image_path_1, True)\n",
        "\n",
        "images_2 = 'https://drive.google.com/uc?id=1VAbEVMKZmKyh9tpe9iVZ_XpTPLpxitwt'\n",
        "image_path_2 = os.path.join(DATA_ROOT, 'images_2.zip')\n",
        "requests.get(images_2)\n",
        "gdown.download(images_2, image_path_2, True)\n",
        "\n",
        "metadata_path = os.path.join(DATA_ROOT, 'metadata.csv')\n",
        "image_path_1 = os.path.join(DATA_ROOT, 'images_1.zip')\n",
        "image_path_2 = os.path.join(DATA_ROOT, 'images_2.zip')\n",
        "images_rgb_path = os.path.join(DATA_ROOT, 'hmnist_8_8_RGB.csv')\n",
        "\n",
        "!pip install patool\n",
        "import patoolib\n",
        "\n",
        "import os.path\n",
        "from os import path\n",
        "\n",
        "if path.exists(os.path.join(DATA_ROOT, 'images_1')) == False:\n",
        "  patoolib.extract_archive(os.path.join(DATA_ROOT, 'images_1.zip'), outdir= os.path.join(DATA_ROOT, 'images_1') )\n",
        "\n",
        "if path.exists(os.path.join(DATA_ROOT, 'images_2')) == False:\n",
        "  patoolib.extract_archive(os.path.join(DATA_ROOT, 'images_2.zip'), outdir= os.path.join(DATA_ROOT, 'images_2') )\n",
        "\n",
        "from distutils.dir_util import copy_tree\n",
        "\n",
        "fromDirectory = os.path.join(DATA_ROOT, 'images_1')\n",
        "toDirectory = os.path.join(DATA_ROOT, 'images_all')\n",
        "\n",
        "copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "fromDirectory = os.path.join(DATA_ROOT, 'images_2')\n",
        "toDirectory = os.path.join(DATA_ROOT, 'images_all')\n",
        "\n",
        "copy_tree(fromDirectory, toDirectory)\n",
        "\n",
        "print(\"Downloaded Data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITfh4GjlttRH"
      },
      "source": [
        "IMG_WIDTH = 100\n",
        "IMG_HEIGHT = 75"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX1w_FIuv26r"
      },
      "source": [
        "#@title Run this to initialize our X, X_g, and y variables { display-mode: \"form\" }\n",
        "metadata = pd.read_csv(metadata_path)\n",
        "metadata['category'] = metadata['dx'].replace({'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'nv': 5, 'vasc': 6,})\n",
        "\n",
        "X = []\n",
        "X_g = []\n",
        "y = []\n",
        "\n",
        "for i in tqdm(range(len(metadata))):\n",
        "  image_meta = metadata.iloc[i]\n",
        "  path = os.path.join(toDirectory, image_meta['image_id'] + '.jpg')\n",
        "  img = cv2.imread(path,cv2.IMREAD_COLOR)\n",
        "  img = cv2.resize(img,(IMG_WIDTH,IMG_HEIGHT))\n",
        "\n",
        "  img_g = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "  X_g.append(img_g)\n",
        "\n",
        "  X.append(img)\n",
        "  y.append(image_meta['category'])\n",
        "\n",
        "X_g = np.array(X_g)\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yvxpG1LB2Ig"
      },
      "source": [
        "sample_cap = 142\n",
        "option = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAZqqMTgnmOq"
      },
      "source": [
        "#@title Option 1: Run this to reduce dataset size. This method caps each class at *sample_cap* samples. { display-mode: \"form\" }\n",
        "if (option == 1):\n",
        "  objects = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
        "  class_totals = [0,0,0,0,0,0,0]\n",
        "  iter_samples = [0,0,0,0,0,0,0]\n",
        "  indicies = []\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    class_totals[y[i]] += 1\n",
        "\n",
        "  print(\"Initial Class Samples\")\n",
        "  print(class_totals)\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    if iter_samples[y[i]] != sample_cap:\n",
        "      indicies.append(i)\n",
        "      iter_samples[y[i]] += 1\n",
        "\n",
        "  X = X[indicies]\n",
        "  X_g = X_g[indicies]\n",
        "\n",
        "  y = y[indicies]\n",
        "\n",
        "  class_totals = [0,0,0,0,0,0,0]\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    class_totals[y[i]] += 1\n",
        "\n",
        "  print(\"Modified Class Samples\")\n",
        "  print(class_totals)\n",
        "else:\n",
        "  print(\"This option was not selected\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7H2PgS99nGdZ"
      },
      "source": [
        "#@title Option 2: Run this to reduce dataset size. This method only reduces the number of *nv* samples to be the same amount as the number of samples found in the second most prevalent class. { display-mode: \"form\" }\n",
        "if (option == 2):\n",
        "  objects = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
        "  class_totals = [0,0,0,0,0,0,0]\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    class_totals[y[i]] += 1\n",
        "\n",
        "  print(\"Initial Class Samples\")\n",
        "  print(class_totals)\n",
        "\n",
        "  largest_index = class_totals.index(max(class_totals))\n",
        "  class_totals[largest_index] = 0\n",
        "\n",
        "  second_largest_val = max(class_totals)\n",
        "\n",
        "  indicies = []\n",
        "  iter = 0\n",
        "  for i in range(len(X)):\n",
        "    if y[i] == largest_index:\n",
        "      if iter != second_largest_val:\n",
        "        indicies.append(i)\n",
        "        iter += 1\n",
        "      else:\n",
        "        continue\n",
        "    else:\n",
        "      indicies.append(i)\n",
        "\n",
        "  class_totals = [0,0,0,0,0,0,0]\n",
        "\n",
        "  for i in range(len(X)):\n",
        "    class_totals[y[i]] += 1\n",
        "\n",
        "  print(\"Modified Class Samples\")\n",
        "  print(class_totals)\n",
        "\n",
        "  X = X[indicies]\n",
        "  X_g = X_g[indicies]\n",
        "\n",
        "  y = y[indicies]\n",
        "else:\n",
        "  print(\"This option was not selected\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC4VupwgyqpN"
      },
      "source": [
        "#@title Run this to Perform Data Augmentation { display-mode: \"form\" }\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)\n",
        "X_g_train, X_g_test, y_train, y_test = train_test_split(X_g, y, test_size=0.4, random_state=101)\n",
        "\n",
        "X_augmented = []\n",
        "X_g_augmented = []\n",
        "\n",
        "y_augmented = []\n",
        "\n",
        "for i in tqdm(range(len(X_train))):\n",
        "  transform = random.randint(0,1)\n",
        "  if (transform == 0):\n",
        "    # Flip the image across the y-axis\n",
        "    X_augmented.append(cv2.flip(X_train[i],1))\n",
        "    X_g_augmented.append(cv2.flip(X_g_train[i],1))\n",
        "    y_augmented.append(y_train[i])\n",
        "  else:\n",
        "    # Zoom 33% into the image\n",
        "    zoom = 0.33\n",
        "\n",
        "    centerX,centerY=int(IMG_HEIGHT/2),int(IMG_WIDTH/2)\n",
        "    radiusX,radiusY= int((1-zoom)*IMG_HEIGHT*2),int((1-zoom)*IMG_WIDTH*2)\n",
        "\n",
        "    minX,maxX=centerX-radiusX,centerX+radiusX\n",
        "    minY,maxY=centerY-radiusY,centerY+radiusY\n",
        "\n",
        "    cropped = (X_train[i])[minX:maxX, minY:maxY]\n",
        "    new_img = cv2.resize(cropped, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    X_augmented.append(new_img)\n",
        "\n",
        "    cropped = (X_g_train[i])[minX:maxX, minY:maxY]\n",
        "    new_img = cv2.resize(cropped, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    X_g_augmented.append(new_img)\n",
        "\n",
        "    y_augmented.append(y_train[i])\n",
        "\n",
        "X_augmented = np.array(X_augmented)\n",
        "X_g_augmented = np.array(X_g_augmented)\n",
        "\n",
        "y_augmented = np.array(y_augmented)\n",
        "\n",
        "X_train = np.vstack((X_train,X_augmented))\n",
        "X_g_train = np.vstack((X_g_train,X_g_augmented))\n",
        "\n",
        "y_train = np.append(y_train,y_augmented)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwVcrhcDDE7h"
      },
      "source": [
        "Let's view the shape of our training variables after data augmentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBBl0Iq5nos-"
      },
      "source": [
        "# Visualizations with Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuJGfWOsud9A"
      },
      "source": [
        "The data for each of our images is very complicated. Recall earlier that when we performed image flattening, each image was represented through a 7500 dimensional feature vector or a 7500 element 1D array. This type of data can be tricky to graph, since we can usually only graph points that have up to 3 dimensions in 3D space. To reduce the dimensionality of our dataset, we can use a method known as Principle Component Analysis, or PCA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH4EyhLJMp7x"
      },
      "source": [
        "We specify which colors we will use to represent each class in the array!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAxZLYq-Mog1"
      },
      "source": [
        "colors = ['red','green','blue','purple','black','brown','orange']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h43Ghnpcw5yM"
      },
      "source": [
        "classes = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWIVZC-D0uOX"
      },
      "source": [
        "X_flat = X.reshape(X.shape[0],X.shape[1]*X.shape[2]*X.shape[3])\n",
        "X_g_flat = X_g.reshape(X_g.shape[0],X_g.shape[1]*X_g.shape[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bAjFDB3Iwcb"
      },
      "source": [
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRj49kQ-GR7Z"
      },
      "source": [
        "Now that we've reduced the dimensionality of our dataset from a 30,000 dimensional feature vector to a 2 dimensional feature vector through PCA, we can plot our points on the scatterplot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Xe7UqSFxuA"
      },
      "source": [
        "plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap=matplotlib.colors.ListedColormap(colors))\n",
        "cb = plt.colorbar()\n",
        "loc = np.arange(0,max(y),max(y)/float(len(colors)))\n",
        "cb.set_ticks(loc)\n",
        "cb.set_ticklabels(classes)\n",
        "plt.title(\"PCA Representation\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8Kmhmb-3a4M"
      },
      "source": [
        "Let's convert this block of code into a function, so its easier to use later on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k3T4o6p3aKG"
      },
      "source": [
        "def plot_visualization(X,y,colors,classes,name):\n",
        "  plt.scatter(X[:,0], X[:,1], c=y, cmap=matplotlib.colors.ListedColormap(colors))\n",
        "  cb = plt.colorbar()\n",
        "  loc = np.arange(0,max(y),max(y)/float(len(colors)))\n",
        "  cb.set_ticks(loc)\n",
        "  cb.set_ticklabels(classes)\n",
        "  plt.title(name)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54SIozasNggc"
      },
      "source": [
        "Now, how does PCA work? Simple put, PCA correlates every variable in our dataset against each other and then tries to remap our values to a reduced number of dimensions, while preserving as much variance (how far off numbers are spread) as possible. It's important to note that the new dimensions created (known as the principle components) often don't have much real world meaning associated with them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekKjyc0_Io8w"
      },
      "source": [
        "Let's try out an alternative dimensionality reduction method known as t-SNE. Perhaps, this method will have better performance. Let's perform the same steps for our t-SNE Representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chtkg0tiIn4f"
      },
      "source": [
        "tsne = TSNE(n_components=2)\n",
        "X_tsne = tsne.fit_transform(X_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CcBXcQxeuYO"
      },
      "source": [
        "plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y, cmap=matplotlib.colors.ListedColormap(colors))\n",
        "cb = plt.colorbar()\n",
        "loc = np.arange(0,max(y),max(y)/float(len(colors)))\n",
        "cb.set_ticks(loc)\n",
        "cb.set_ticklabels(classes)\n",
        "plt.title(\"T-SNE Representation\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op9wmtS9QsAX"
      },
      "source": [
        "The mathematics behind how t-SNE works is much more complicated (check out [this](https://towardsdatascience.com/t-sne-clearly-explained-d84c537f53a) link). For now, as long as we know generally what these ML methods are doing, we should be fine to implement them in many applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkz1SPCeLOtP"
      },
      "source": [
        "However, in our case it seems that this visualization isn't the most helpful, as it is hard to discern a hard boundry between each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rclIM8a6IE4U"
      },
      "source": [
        "# Computer Vision Features for Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAbuYLfbWaZe"
      },
      "source": [
        "In previous notebooks, we tried out several methods for classifying our skin cancer lesions. Let's try out another method. If this method has greater performance, our web app will be even better!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lb2RO6psr5x"
      },
      "source": [
        "lesion_img = X_g[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSgI1Dy7IJ9j"
      },
      "source": [
        "Images contain specific pixel configurations and characteristics that can be discerned through computer vision feature detectors. These features can be used for a variety of tasks, from identifiying if an image has been doctored, or to identify facial key points.\n",
        "\n",
        "In our case, we'll can actually use these computer vision features as data to train classifiers! Essentially, we'll be respresenting images through the features that have been detected in it, and feeding the features into ML algorithms for training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bliG9uyigzQ"
      },
      "source": [
        "![alt text](https://i.stack.imgur.com/bpaz6.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-B7wVfAESi3D"
      },
      "source": [
        "We'll specifically be using SIFT features. Scale-invariant feature transform, or SIFT, is a feature detection algorithm that identifies interesting features in an image. Unlike CNNs, which use convolutional layers to discern image features, SIFT detectors use gradients, or how an image's color changes in respect to direction. Just as a convolutional kernal calculates values for each pixel in an image, gradient calculators can do the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HOc0bckTao0"
      },
      "source": [
        "![alt text](https://www.codeproject.com/KB/recipes/619039/SIFT.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBNNnVWLjLIj"
      },
      "source": [
        "Gradients are very useful for performing feature extraction, as areas of an image in which the color intensity abruptly changes, can indicate the possible presence of a key point. If we were presented with a satellitle image of an ocean with a boat, the boat would likely be detected with gradients, due to the change in color intensity between the boat and the ocean water."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfrmensBlT9N"
      },
      "source": [
        "![alt text](\n",
        "https://ak.picdn.net/shutterstock/videos/22004455/thumb/11.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyjH7tUnk_p_"
      },
      "source": [
        "In many cases, gradients can be used to perform edge detection. In the image of the building above, we can see that a lot of the features are detected at edges of the building."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrdvQbhxOLx1"
      },
      "source": [
        "We can detect SIFT images in an image by using the following code. An interesting item to note is that the SIFT method has been patented, so we have to use the `opencv-contrib` library, as opposed to the regular `opencv` library to access the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaQ_Xuk9IJi2"
      },
      "source": [
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "keypoints, descriptor = sift.detectAndCompute(lesion_img,None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rxp4G90st9Y"
      },
      "source": [
        "We can then display the keypoint with the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7QDzSeNsxpH"
      },
      "source": [
        "sift_img = cv2.drawKeypoints(X[0],keypoints,lesion_img)\n",
        "cv2_imshow(sift_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd12Vtr7OaPR"
      },
      "source": [
        "There are two components to each SIFT feature, keypoints and descriptors. Keypoints are the location for the SIFT feature while descriptors contain more qualitative information regarding the feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR1jIGjBPDR5"
      },
      "source": [
        "print(descriptor.shape)\n",
        "print(descriptor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXr4MP34nRe7"
      },
      "source": [
        "The number of SIFT features detected varies from image to image, but the descriptor shape remains the same, a 128 dimensional vector.\n",
        "\n",
        "Now, if we were to represent images through their descriptors, we could train traditional Sci-kit Learn models for classification.\n",
        "\n",
        "Let's start building our classifier. We'll first need to detect all the SIFT features in our images and store them in a `all_descriptors`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUS3ZQwznq3r"
      },
      "source": [
        "all_descriptors = []\n",
        "\n",
        "for i in tqdm(range(X_train.shape[0])):\n",
        "  kp, des = sift.detectAndCompute(X_train[i], None)\n",
        "\n",
        "  if des is not None:\n",
        "    for d in des:\n",
        "      all_descriptors.append(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTQogQSHpXIk"
      },
      "source": [
        "Looks like we've found a lot of features!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKvtmYRmpWof"
      },
      "source": [
        "print(len(all_descriptors))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA9cGaR4Uf7L"
      },
      "source": [
        "Right now, each image that the SIFT feature detector operates on, has a different number of image features detected. We need to know how we can transform this data into something that we could feed a Sci-kit Learn classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kBxLphgo0a4"
      },
      "source": [
        "How would we do this? Well, we'll actually implement a technique from NLP here! Just as we used vectors of words to represent sentences in the bag of words method, now, we'll use descriptors (visual words) to represent our images (sentences). Just as we had to with NLP, we'll define a vocabulary size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut20B0FIVt2F"
      },
      "source": [
        "In this case, we'll make it 10 times the number of classes in our dataset. Once we've detected all the SIFT features in all of our images, we can perform K means clustering and transform the shape of our data. K means clustering is a method of generating labels for data points based on their similarities to other data points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-hv2cPGXof1"
      },
      "source": [
        "Let's get started?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2ksRzKFr3jp"
      },
      "source": [
        "k = len(classes)*10\n",
        "\n",
        "sift_kmeans = KMeans(n_clusters=k)\n",
        "sift_kmeans.fit(all_descriptors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTJA7r_8J4dF"
      },
      "source": [
        "Performing K Means computations for our training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC_fjZvzo1Fd"
      },
      "source": [
        "#@title Performing K Means Computations for Training { display-mode: \"form\" }\n",
        "X_sift_train = []\n",
        "y_sift_train = []\n",
        "\n",
        "for i in tqdm(range(X_train.shape[0])):\n",
        "  kp, des = sift.detectAndCompute(X_train[i], None)\n",
        "\n",
        "  sift_sample = np.zeros(k)\n",
        "  nkp = np.size(kp)\n",
        "\n",
        "  if des is not None:\n",
        "    for d in des:\n",
        "      idx = sift_kmeans.predict([d])\n",
        "      sift_sample[idx] += 1/nkp\n",
        "\n",
        "    X_sift_train.append(sift_sample)\n",
        "    y_sift_train.append(y_train[i])\n",
        "\n",
        "X_sift_train = np.array(X_sift_train)\n",
        "y_sift_train = np.array(y_sift_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP48xQ4KJ-D7"
      },
      "source": [
        "Performing K Means computations for our testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJXvU37j1dyw"
      },
      "source": [
        "X_sift_test = []\n",
        "y_sift_test = []\n",
        "\n",
        "for i in tqdm(range(X_test.shape[0])):\n",
        "  kp, des = sift.detectAndCompute(X_test[i], None)\n",
        "\n",
        "  sift_sample = np.zeros(k)\n",
        "  nkp = np.size(kp)\n",
        "\n",
        "  if des is not None:\n",
        "    for d in des:\n",
        "      idx = sift_kmeans.predict([d])\n",
        "      sift_sample[idx] += 1/nkp\n",
        "\n",
        "    X_sift_test.append(sift_sample)\n",
        "    y_sift_test.append(y_train[i])\n",
        "\n",
        "X_sift_test = np.array(X_sift_test)\n",
        "y_sift_test = np.array(y_sift_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnGLKiYxsSx3"
      },
      "source": [
        "Now that we've made our X and y variables, lets train a classifier. Below, we'll define a neural network. However, we can try out other models as well, as our dataset is now preprocessed and compatible with Scikit-learn!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDz-_m1hsbUG"
      },
      "source": [
        "sift_mlp = MLPClassifier(random_state=101, max_iter=900000)\n",
        "sift_mlp.fit(X_sift_train,y_sift_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfUoNNAxs6aQ"
      },
      "source": [
        "Let's view our model's performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdgFEDo82dYQ"
      },
      "source": [
        "#@title Definition for the model_stats() function from our first notebook { display-mode: \"form\" }\n",
        "def model_stats(name, y_test, y_pred, y_pred_proba):\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  print(name)\n",
        "\n",
        "  accuracy = accuracy_score(y_test,y_pred)\n",
        "  print (\"The accuracy of the model is \" + str(round(accuracy,5)))\n",
        "\n",
        "  roc_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovo')\n",
        "\n",
        "  print (\"The ROC AUC Score of the model is \" + str(round(roc_score,5)))\n",
        "\n",
        "  return cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4dV5WiZttAQ"
      },
      "source": [
        "y_pred = sift_mlp.predict(X_sift_test)\n",
        "y_pred_proba = sift_mlp.predict_proba(X_sift_test)\n",
        "\n",
        "sift_cm = model_stats(\"SIFT MLP Model\",y_sift_test,y_pred,y_pred_proba)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS-p2b20u6nM"
      },
      "source": [
        "Let's take a look at the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHweJLDq2hJO"
      },
      "source": [
        "#@title Definition for the plot_cm() function from our first notebook { display-mode: \"form\" }\n",
        "def plot_cm(name, cm):\n",
        "  classes = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
        "\n",
        "  df_cm = pd.DataFrame(cm, index = [i for i in classes], columns = [i for i in classes])\n",
        "  df_cm = df_cm.round(5)\n",
        "\n",
        "  plt.figure(figsize = (12,8))\n",
        "  sns.heatmap(df_cm, annot=True, fmt='g')\n",
        "  plt.title(name + \" Model Confusion Matrix\")\n",
        "  plt.xlabel(\"Predicted Label\")\n",
        "  plt.ylabel(\"True Label\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDTNCB6nvF05"
      },
      "source": [
        "classes = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
        "\n",
        "df_cm = pd.DataFrame(sift_cm, index = [i for i in classes], columns = [i for i in classes])\n",
        "df_cm = df_cm.round(5)\n",
        "\n",
        "plt.figure(figsize = (12,8))\n",
        "sns.heatmap(df_cm, annot=True, fmt='g')\n",
        "plt.title(\"SIFT Model Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9cvye8tvt2w"
      },
      "source": [
        "Looks like using the computer vision features for classification didn't work out well in this context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PayhcQD9v3mO"
      },
      "source": [
        "It's important to note however, that even if a method doesn't work well with one dataset, that it may work well with another dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGHRcoed24g1"
      },
      "source": [
        "Now, let's combine our dataset back together so we can make some visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_673XUY2zp_"
      },
      "source": [
        "X_sift = np.vstack((X_sift_train,X_sift_test))\n",
        "y_sift = np.append(y_sift_train,y_sift_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar61nVInwHRx"
      },
      "source": [
        "Let's take a look at the t-SNE representation of our image vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQv_K5qX3IDS"
      },
      "source": [
        "sift_tsne = TSNE(n_components=2)\n",
        "X_sift_tsne = sift_tsne.fit_transform(X_sift)\n",
        "\n",
        "colors = ['red','green','blue','purple','black','brown','orange']\n",
        "classes = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
        "\n",
        "plot_visualization(X_sift_tsne,y_sift,colors,classes,\"T-SNE Representation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8nyLuWlwtvs"
      },
      "source": [
        "It doesn't look like the SIFT feature method differentiated the data very well. In ML projects, we'll be faced with this a lot. We'll have to test out multiple methods to see which performs best, and not all will perform very well. However, exploring all these new techniques and cool visualizations can still be very fun!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpSs2j6WaptK"
      },
      "source": [
        "There are also other features for which OpenCV has feature detector functions. These features include SURF, FAST, BRISK, BRIEF, and ORB. Each feature has its own unique characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNijk1YMi2Cl"
      },
      "source": [
        "In summary, our bag of words based SIFT classifier worked as follows.\n",
        "\n",
        "![alt text](https://www.researchgate.net/publication/334093328/figure/fig3/AS:774765572796416@1561729915048/A-diagram-illustrating-how-to-generate-the-bag-of-words-visual-words-from-several.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvdOt-LFrEQm"
      },
      "source": [
        "# Lesion Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5ELaKS3rIPt"
      },
      "source": [
        "Now that we've used t-SNE and PCA for dimensionality reduction, let's try to see if we can segment the lesions out of the images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qZC2Ys9FQBh"
      },
      "source": [
        "If we performlesion segmentation we could prevent bias being introduced into our model based on characteristics of the background. Skin color, other moles, or hair would be prevented from having an impact in the classification.\n",
        "\n",
        "To perform this task we'll try out multiple methods. We'll start off with basic OpenCV based thresholding, move onto clustering, and then implement complex U-net models. Let's implement otsu binarization on an image and see what the segmentation looks like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7Al5FIszF39"
      },
      "source": [
        "Let's specify how many images we want to test. For these images, we'll need to maunally validate whether the lesion segmentation was accurate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJyNR4L-zNv6"
      },
      "source": [
        "IMGS_TO_CHECK = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY3V2s-NFXYc"
      },
      "source": [
        "from ipywidgets import interact\n",
        "import ipywidgets as widgets\n",
        "#@title Otsu Binarization { display-mode: \"form\" }\n",
        "def plot_thing(image_index):\n",
        "  lesion_img = X_g[image_index]\n",
        "  ret2,otsu_img = cv2.threshold(lesion_img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "  cv2_imshow(X[image_index])\n",
        "  cv2_imshow(otsu_img)\n",
        "interact(plot_thing, image_index=widgets.IntSlider(min=0, max= IMGS_TO_CHECK - 1, step=1))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBWgtrX9tboO"
      },
      "source": [
        "Let's try some unsupervised methods to perform lesion segmentation. Specifically we'll attempt using two clustering algorithms.\n",
        "\n",
        "K means clustering and Agglomerative clustering are two forms of unsupervised learning, that generate labels to place on unlabeled data points based on their characteristics. We will be treating each pixel as a datapoint and allow the unsupervised algorithms to attempt to classify each pixel as belonging to the foreground (lesion) or background (remaining skin)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlcur0uIv32-"
      },
      "source": [
        "print(\"K Means Clustering\")\n",
        "kmlist = []\n",
        "for i in range(IMGS_TO_CHECK):\n",
        "  lesion_img = X[i]\n",
        "\n",
        "  lesion_img_flat = lesion_img.reshape(lesion_img.shape[0]*lesion_img.shape[1],lesion_img.shape[2])\n",
        "\n",
        "  kmeans = KMeans(n_clusters=2, random_state=101)\n",
        "  kmeans_labels = kmeans.fit_predict(lesion_img_flat)\n",
        "\n",
        "  for j in range(len(kmeans_labels)):\n",
        "    if kmeans_labels[j] == 1:\n",
        "      kmeans_labels[j] = 255\n",
        "\n",
        "  kmeans_lesion_img = kmeans_labels.reshape(IMG_HEIGHT,IMG_WIDTH)\n",
        "  kmlist.append(kmeans_lesion_img)\n",
        "\n",
        "def plot_thing(image_index):\n",
        "  lesion_img = X_g[image_index]\n",
        "  kmeans_lesion_img = kmlist[image_index]\n",
        "  cv2_imshow(lesion_img)\n",
        "  cv2_imshow(kmeans_lesion_img)\n",
        "\n",
        "interact(plot_thing, image_index=widgets.IntSlider(min=0, max=IMGS_TO_CHECK - 1, step=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZXA9YYt2NBz"
      },
      "source": [
        "Let's take a look at the t-SNE representation of one of the images with the labeled clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yeo6M9ZH5NRh"
      },
      "source": [
        "lesion_img = X[0]\n",
        "lesion_img_flat = lesion_img.reshape(lesion_img.shape[0]*lesion_img.shape[1],lesion_img.shape[2])\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=101)\n",
        "kmeans_labels = kmeans.fit_predict(lesion_img_flat)\n",
        "\n",
        "for i in range(len(kmeans_labels)):\n",
        "  if kmeans_labels[i] == 1:\n",
        "    kmeans_labels[i] = 255\n",
        "\n",
        "tsne = TSNE(n_components=2)\n",
        "lesion_img_tsne = tsne.fit_transform(lesion_img_flat)\n",
        "\n",
        "colors = ['red', 'blue']\n",
        "classes = [0,1]\n",
        "\n",
        "plot_visualization(lesion_img_tsne,kmeans_labels,colors,classes,\"T-SNE K Means Clustering Representation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co9sCK4m0DsD"
      },
      "source": [
        "Let's try out the same unsupervised learning method but with agglomerative clustering instead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8no7ew-5SXy"
      },
      "source": [
        "print(\"Agglomerative Clustering\")\n",
        "aglist = []\n",
        "for i in range(IMGS_TO_CHECK):\n",
        "  lesion_img = X[i]\n",
        "\n",
        "  lesion_img_flat = lesion_img.reshape(lesion_img.shape[0]*lesion_img.shape[1],lesion_img.shape[2])\n",
        "\n",
        "  agglomerative = AgglomerativeClustering(n_clusters=2)\n",
        "  agglomerative_labels = agglomerative.fit_predict(lesion_img_flat)\n",
        "\n",
        "  for j in range(len(agglomerative_labels)):\n",
        "    if agglomerative_labels[j] == 1:\n",
        "      agglomerative_labels[j] = 255\n",
        "\n",
        "  agglomerative_lesion_img = agglomerative_labels.reshape(IMG_HEIGHT,IMG_WIDTH)\n",
        "  aglist.append(agglomerative_lesion_img)\n",
        "\n",
        "def plot_thing(image_index):\n",
        "  lesion_img = X_g[image_index]\n",
        "  agglomerative_lesion_img = aglist[image_index]\n",
        "  cv2_imshow(lesion_img)\n",
        "  cv2_imshow(agglomerative_lesion_img)\n",
        "\n",
        "interact(plot_thing, image_index=widgets.IntSlider(min=0, max=IMGS_TO_CHECK - 1, step=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiXaDcXh4GHy"
      },
      "source": [
        "Let's take a look at the t-SNE representation of one of these images as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOEzjQsI4Jlo"
      },
      "source": [
        "lesion_img = X[0]\n",
        "lesion_img_flat = lesion_img.reshape(lesion_img.shape[0]*lesion_img.shape[1],lesion_img.shape[2])\n",
        "\n",
        "agglomerative = AgglomerativeClustering(n_clusters=2)\n",
        "agglomerative_labels = agglomerative.fit_predict(lesion_img_flat)\n",
        "\n",
        "for i in range(len(agglomerative_labels)):\n",
        "  if agglomerative_labels[i] == 1:\n",
        "    agglomerative_labels[i] = 255\n",
        "\n",
        "tsne = TSNE(n_components=2)\n",
        "lesion_img_tsne = tsne.fit_transform(lesion_img_flat)\n",
        "\n",
        "colors = ['red', 'blue']\n",
        "classes = [0,1]\n",
        "\n",
        "plot_visualization(lesion_img_tsne,agglomerative_labels,colors,classes,\"T-SNE Agglomerative Clustering Representation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUi3OhyG4tfj"
      },
      "source": [
        "Seems like these unsupervised learning methods are a bit hit or miss, and can get confused easily. Let's try using Keras and CNNs to develop a lesion segmentation model. Perhaps this method will perform better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvacPEFKQi_Z"
      },
      "source": [
        "#@title Run this to download our dataset for Image Segmentation { display-mode: \"form\" }\n",
        "os.makedirs('images_seg', exist_ok=True)\n",
        "!wget -O images_seg.zip 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20B)%20Skin%20Cancer%20Diagnosis/images_seg.zip'\n",
        "!unzip -q images_seg.zip -d images_seg\n",
        "\n",
        "os.makedirs('segmentation', exist_ok=True)\n",
        "!wget -O segmentations.zip 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20(Healthcare%20B)%20Skin%20Cancer%20Diagnosis/segmentations.zip'\n",
        "!unzip -q segmentations.zip -d segmentation\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUqi2yOXTzx9"
      },
      "source": [
        "IMG_SEG_WIDTH = 256\n",
        "IMG_SEG_HEIGHT = 192"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDCmXeTScqPo"
      },
      "source": [
        "Let's seperate our dataset into an `X` and `Y`. As we've already used those variable names, we'll define our variables as `X_seg` and `y_seg`. Remember, X represents our input data and output data.\n",
        "\n",
        "Unlike classification, where our input data are images, and our output data are labels, both our input and output data are images for segmentation. Specifically, our X has color images of skin lesions, while our Y has black and white \"masks.\" These \"masks\" have the lesion segmented in white, while background is made the be black."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOUAi97ySnoK",
        "cellView": "form"
      },
      "source": [
        "#@title Run this to create our X_seg and y_seg variables\n",
        "\n",
        "X_seg = []\n",
        "y_seg = []\n",
        "\n",
        "path, dirs, files = next(os.walk(\"images_seg/Images\"))\n",
        "\n",
        "path, dirs, files_seg = next(os.walk(\"segmentation/Segmentation\"))\n",
        "\n",
        "for i in tqdm(range(len(files))):\n",
        "  file_name = files[i].split('.')[0]\n",
        "  seg_index = [j for j, s in enumerate(files_seg) if file_name in s][0]\n",
        "\n",
        "  img = cv2.imread('images_seg/Images/' + files[i],cv2.IMREAD_COLOR)\n",
        "  img = cv2.resize(img,(IMG_SEG_WIDTH,IMG_SEG_HEIGHT))\n",
        "  img = img/255.0\n",
        "  X_seg.append(img)\n",
        "\n",
        "  img = cv2.imread('segmentation/Segmentation/' + files_seg[seg_index],cv2.IMREAD_COLOR)\n",
        "  img = cv2.resize(img,(IMG_SEG_WIDTH,IMG_SEG_HEIGHT))\n",
        "  img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "  img = img/255.0\n",
        "  y_seg.append(img)\n",
        "\n",
        "X_seg = np.array(X_seg)\n",
        "y_seg = np.array(y_seg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUp-hBFBeuBE"
      },
      "source": [
        "Let's take a look at an image, and its mask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aNe6gHQeFCO"
      },
      "source": [
        "print(\"Original Image\")\n",
        "cv2_imshow(X_seg[2]*255)\n",
        "print(\"Image Mask\")\n",
        "cv2_imshow(y_seg[2]*255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6wH4vlFfAMj"
      },
      "source": [
        "Let's perform our test/train split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv52CAZHVoDD"
      },
      "source": [
        "X_seg_train, X_seg_test, y_seg_train, y_seg_test = train_test_split(X_seg, y_seg, test_size=0.2, random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J_QKmBffdnW"
      },
      "source": [
        "We'll be using a model known as a U-net to perform our image segmentation. How do U-nets work? Let's take a look at the architecture. On the left hand side, we can see the image dimensions decreasing. This segment is present in the CNN we used earlier for classification as well, as a lot of the feature extraction occurs here. However with U-nets, we perform *up-convolutions* to increase our image resolution once again, to create a two-color image map. White represents our foreground while black represents our background. We connect the *contraction* segment on the left with the *expansion* segment on the right with the horizontal *bottleneck* segment. This segment selects the \"important\" features from the *contraction* stage so that they can be used to upscale a better high resolution image segmentation map in the *expansion* stage.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/1225/1*f7YOaE4TWubwaFF7Z1fzNw.png)\n",
        "\n",
        "\n",
        "\n",
        "U-nets are able to able to perform segmentation well, because in practice, they are essentially classifying every image pixel to be a part of the background or foreground. For more information regarding U-nets and the model we use, check out [this](https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5) link.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vyFIIvxasuq"
      },
      "source": [
        "#@title Run this code segment to define our model\n",
        "def build_model():\n",
        "  inputs = Input((IMG_SEG_HEIGHT, IMG_SEG_WIDTH, 3))\n",
        "  s = Lambda(lambda x: x / 255) (inputs)\n",
        "\n",
        "  conv_blocks = [16,32,64,128,256,128,64,32,16]\n",
        "\n",
        "  conv1 = Conv2D(conv_blocks[0], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\n",
        "  conv1 = Dropout(0.1) (conv1)\n",
        "  conv1 = Conv2D(conv_blocks[0], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv1)\n",
        "  pool1 = MaxPooling2D((2, 2)) (conv1)\n",
        "\n",
        "  conv2 = Conv2D(conv_blocks[1], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (pool1)\n",
        "  conv2 = Dropout(0.1) (conv2)\n",
        "  conv2 = Conv2D(conv_blocks[1], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv2)\n",
        "  pool2 = MaxPooling2D((2, 2)) (conv2)\n",
        "\n",
        "  conv3 = Conv2D(conv_blocks[2], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (pool2)\n",
        "  conv3 = Dropout(0.2) (conv3)\n",
        "  conv3 = Conv2D(conv_blocks[2], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv3)\n",
        "  pool3 = MaxPooling2D((2, 2)) (conv3)\n",
        "\n",
        "  conv4 = Conv2D(conv_blocks[3], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (pool3)\n",
        "  conv4 = Dropout(0.2) (conv4)\n",
        "  conv4 = Conv2D(conv_blocks[3], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv4)\n",
        "  pool4 = MaxPooling2D(pool_size=(2, 2)) (conv4)\n",
        "\n",
        "  conv5 = Conv2D(conv_blocks[4], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (pool4)\n",
        "  conv5 = Dropout(0.3) (conv5)\n",
        "  conv5 = Conv2D(conv_blocks[4], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv5)\n",
        "\n",
        "  upconv6 = Conv2DTranspose(conv_blocks[5], (2, 2), strides=(2, 2), padding='same') (conv5)\n",
        "  upconv6 = concatenate([upconv6, conv4])\n",
        "  conv6 = Conv2D(conv_blocks[5], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (upconv6)\n",
        "  conv6 = Dropout(0.2) (conv6)\n",
        "  conv6 = Conv2D(conv_blocks[5], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv6)\n",
        "\n",
        "  upconv7 = Conv2DTranspose(conv_blocks[6], (2, 2), strides=(2, 2), padding='same') (conv6)\n",
        "  upconv7 = concatenate([upconv7, conv3])\n",
        "  conv7 = Conv2D(conv_blocks[6], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (upconv7)\n",
        "  conv7 = Dropout(0.2) (conv7)\n",
        "  conv7 = Conv2D(conv_blocks[6], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv7)\n",
        "\n",
        "  upconv8 = Conv2DTranspose(conv_blocks[7], (2, 2), strides=(2, 2), padding='same') (conv7)\n",
        "  upconv8 = concatenate([upconv8, conv2])\n",
        "  conv8 = Conv2D(conv_blocks[7], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (upconv8)\n",
        "  conv8 = Dropout(0.1) (conv8)\n",
        "  conv8 = Conv2D(conv_blocks[7], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv8)\n",
        "\n",
        "  upconv9 = Conv2DTranspose(conv_blocks[8], (2, 2), strides=(2, 2), padding='same') (conv8)\n",
        "  upconv9 = concatenate([upconv9, conv1], axis=3)\n",
        "  conv9 = Conv2D(conv_blocks[8], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (upconv9)\n",
        "  conv9 = Dropout(0.1) (conv9)\n",
        "  conv9 = Conv2D(conv_blocks[8], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv9)\n",
        "\n",
        "  outputs = Conv2D(1, (1, 1), activation='sigmoid') (conv9)\n",
        "\n",
        "  model = Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_j-Ti6rflDA"
      },
      "source": [
        "The performance metric for image segmentation scenarios is IOU, or Intersection Over Union. This metric quantifies the percent overlap between the mask and our predicted output. An IOU value > 0.5 is considered to represent good performance.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/600/0*kraYHnYpoJOhaMzq.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp-ImlbjYWx0"
      },
      "source": [
        "As Keras doesn't have this metric, we will define a function that performs this calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWsZMLIwa7Iq"
      },
      "source": [
        "def iou(y_true, y_pred):\n",
        "     def f(y_true, y_pred):\n",
        "         intersection = (y_true * y_pred).sum()\n",
        "         union = y_true.sum() + y_pred.sum() - intersection\n",
        "         x = (intersection + 1e-15) / (union + 1e-15)\n",
        "         x = x.astype(np.float32)\n",
        "         return x\n",
        "     return tf.numpy_function(f, [y_true, y_pred], tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXMU70kz7bGA"
      },
      "source": [
        "We'll also set some hyperparameters for our model here and compile. Note: if we get lots of errors from Tensorflow, it's okay!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3XHXe2MbBYI"
      },
      "source": [
        "## Hyperparameters\n",
        "batch = 16\n",
        "lr = 1e-4\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(lr)\n",
        "metrics = [iou]\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj-uyaU7gMiM"
      },
      "source": [
        "Let's train our model on our image segmentation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o9vg_NIY0r4"
      },
      "source": [
        "model.fit(X_seg_train.astype(np.float32), y_seg_train.astype(np.float32),\n",
        "        validation_data=(X_seg_test.astype(np.float32),y_seg_test.astype(np.float32))\n",
        "        ,epochs=20,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zA0WT6HYgQe0"
      },
      "source": [
        "Let's view our model's performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO4Pj5BVgaLG"
      },
      "source": [
        "y_seg_pred = model.predict(X_seg_test)\n",
        "iou_val = iou(np.expand_dims(y_seg_test,axis=3),y_seg_pred)\n",
        "print(\"IOU: \" + str(iou_val.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlfgN29Qmmlr"
      },
      "source": [
        "Let's take a look at a segmentation from the testing dataset for our U-net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r40__nJmmHcY"
      },
      "source": [
        "image_index = 27"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drWaG4lNQsR5"
      },
      "source": [
        "Original Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTqBJnVDQoAj"
      },
      "source": [
        "print(\"Original Image\")\n",
        "cv2_imshow(X_seg_test[image_index]*255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuSi8GfEQtlZ"
      },
      "source": [
        "True Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRUJufEOQpVZ"
      },
      "source": [
        "print(\"True Segmentation\")\n",
        "cv2_imshow(y_seg_test[image_index]*255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NasuX1kkQvWx"
      },
      "source": [
        "Predicted Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC43OIsJYmnU"
      },
      "source": [
        "print(\"Predicted Segmentation\")\n",
        "prediction = model.predict(X_seg_test[image_index][None,:])\n",
        "\n",
        "prediction_img = np.reshape(prediction.flatten(),(IMG_SEG_HEIGHT, IMG_SEG_WIDTH))\n",
        "retval, threshold = cv2.threshold(prediction_img, 0.5, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "cv2_imshow(threshold*255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7FdKZzo7nBt"
      },
      "source": [
        "Great! Looks like our U-net model is pretty decent at performing lesion segmentation. The U-net is definately better than our previous approaches for performing lesion segmentation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53a9JXBZK5IT"
      },
      "source": [
        "Let's build our U-Net model with Keras!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQeuVu4teAQH"
      },
      "source": [
        "def build_model_modified():\n",
        "  inputs = Input((IMG_SEG_HEIGHT, IMG_SEG_WIDTH, 3))\n",
        "  s = Lambda(lambda x: x / 255) (inputs)\n",
        "\n",
        "  # Code below is a copy from the original model code from above as placeholder.\n",
        "\n",
        "  conv_blocks = [16,32,64,128,256,128,64,32,16]\n",
        "\n",
        "  conv1 = Conv2D(conv_blocks[0], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\n",
        "  conv1 = Dropout(0.1) (conv1)\n",
        "  conv1 = Conv2D(conv_blocks[0], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv1)\n",
        "  pool1 = MaxPooling2D((2, 2)) (conv1)\n",
        "\n",
        "  conv2 = Conv2D(conv_blocks[1], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (pool1)\n",
        "  conv2 = Dropout(0.1) (conv2)\n",
        "  conv2 = Conv2D(conv_blocks[1], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv2)\n",
        "  pool2 = MaxPooling2D((2, 2)) (conv2)\n",
        "\n",
        "  conv3 = Conv2D(conv_blocks[2], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (pool2)\n",
        "  conv3 = Dropout(0.2) (conv3)\n",
        "  conv3 = Conv2D(conv_blocks[2], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv3)\n",
        "  pool3 = MaxPooling2D((2, 2)) (conv3)\n",
        "\n",
        "  conv4 = Conv2D(conv_blocks[3], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (pool3)\n",
        "  conv4 = Dropout(0.2) (conv4)\n",
        "  conv4 = Conv2D(conv_blocks[3], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv4)\n",
        "  pool4 = MaxPooling2D(pool_size=(2, 2)) (conv4)\n",
        "\n",
        "  conv5 = Conv2D(conv_blocks[4], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (pool4)\n",
        "  conv5 = Dropout(0.3) (conv5)\n",
        "  conv5 = Conv2D(conv_blocks[4], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv5)\n",
        "\n",
        "  upconv6 = Conv2DTranspose(conv_blocks[5], (2, 2), strides=(2, 2), padding='same') (conv5)\n",
        "  upconv6 = concatenate([upconv6, conv4])\n",
        "  conv6 = Conv2D(conv_blocks[5], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (upconv6)\n",
        "  conv6 = Dropout(0.2) (conv6)\n",
        "  conv6 = Conv2D(conv_blocks[5], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv6)\n",
        "\n",
        "  upconv7 = Conv2DTranspose(conv_blocks[6], (2, 2), strides=(2, 2), padding='same') (conv6)\n",
        "  upconv7 = concatenate([upconv7, conv3])\n",
        "  conv7 = Conv2D(conv_blocks[6], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (upconv7)\n",
        "  conv7 = Dropout(0.2) (conv7)\n",
        "  conv7 = Conv2D(conv_blocks[6], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv7)\n",
        "\n",
        "  upconv8 = Conv2DTranspose(conv_blocks[7], (2, 2), strides=(2, 2), padding='same') (conv7)\n",
        "  upconv8 = concatenate([upconv8, conv2])\n",
        "  conv8 = Conv2D(conv_blocks[7], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (upconv8)\n",
        "  conv8 = Dropout(0.1) (conv8)\n",
        "  conv8 = Conv2D(conv_blocks[7], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv8)\n",
        "\n",
        "  upconv9 = Conv2DTranspose(conv_blocks[8], (2, 2), strides=(2, 2), padding='same') (conv8)\n",
        "  upconv9 = concatenate([upconv9, conv1], axis=3)\n",
        "  conv9 = Conv2D(conv_blocks[8], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (upconv9)\n",
        "  conv9 = Dropout(0.1) (conv9)\n",
        "  c9 = Conv2D(conv_blocks[8], (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv9)\n",
        "\n",
        "  ###\n",
        "\n",
        "  outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
        "\n",
        "  model = Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4eIdMVTLbu9"
      },
      "source": [
        "#@title Compile and set some Hyperparameters here { display-mode: \"form\" }\n",
        "\n",
        "## Hyperparameters\n",
        "batch = 16\n",
        "lr = 1e-4\n",
        "\n",
        "model = build_model_modified()\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(lr)\n",
        "metrics = [iou]\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et25mb7ZLcBK"
      },
      "source": [
        "#@title Train Our Model Here { display-mode: \"form\" }\n",
        "model.fit(X_seg_train.astype(np.float32), y_seg_train.astype(np.float32),\n",
        "        validation_data=(X_seg_test.astype(np.float32),y_seg_test.astype(np.float32))\n",
        "        ,epochs=20,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TVX8eV1LdsK"
      },
      "source": [
        "#@title Evaluate Model Performance Here { display-mode: \"form\" }\n",
        "y_seg_pred = model.predict(X_seg_test)\n",
        "iou_val = iou(np.expand_dims(y_seg_test,axis=3),y_seg_pred)\n",
        "print(\"IOU: \" + str(iou_val.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn6879StLkmC"
      },
      "source": [
        "#@title View an Individual Segmentation Here { display-mode: \"form\" }\n",
        "image_index = 45\n",
        "\n",
        "print(\"Original Image\")\n",
        "cv2_imshow(X_seg_test[image_index]*255)\n",
        "\n",
        "print(\"True Segmentation\")\n",
        "cv2_imshow(y_seg_test[image_index]*255)\n",
        "\n",
        "print(\"Predicted Segmentation\")\n",
        "prediction = model.predict(X_seg_test[image_index][None,:])\n",
        "\n",
        "prediction_img = np.reshape(prediction.flatten(),(IMG_SEG_HEIGHT, IMG_SEG_WIDTH))\n",
        "retval, threshold = cv2.threshold(prediction_img, 0.5, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "cv2_imshow(threshold*255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A9J3SW4nMZL"
      },
      "source": [
        "In this notebook, we explored multiple dimensionality reduction techniques to better understand our dataset. This can helps in determining what steps to take when developing and tweaking additional ML models.\n",
        "\n",
        "Our work with SIFT feature based classification and U-net image segmentation is also very helpful. We could continue tweaking  our SIFT feature based classification model to eventually integrate it as part of an ensemble classification system! This would mean that the SIFT based classification model would be one of many different types of ML models being sampled during classification. This ensemble system could have greater skin cancer classification performance, and some of our next steps could be to integrate this into our web app! We could also integrate our U-net image segmentation model as a part of the skin cancer diagnosis pipline in our web app, by first performing image segmentation on the users device, prior to the actual classification by our CNN."
      ]
    }
  ]
}